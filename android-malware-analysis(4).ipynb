{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":587,"sourceType":"datasetVersion","datasetId":272},{"sourceId":680743,"sourceType":"datasetVersion","datasetId":344265},{"sourceId":12681097,"sourceType":"datasetVersion","datasetId":8013836},{"sourceId":12681244,"sourceType":"datasetVersion","datasetId":8013939},{"sourceId":12683862,"sourceType":"datasetVersion","datasetId":8015789},{"sourceId":12693467,"sourceType":"datasetVersion","datasetId":8021926},{"sourceId":12693489,"sourceType":"datasetVersion","datasetId":8021941},{"sourceId":12693507,"sourceType":"datasetVersion","datasetId":8021956},{"sourceId":12700707,"sourceType":"datasetVersion","datasetId":8026691},{"sourceId":12700722,"sourceType":"datasetVersion","datasetId":8026702},{"sourceId":12703729,"sourceType":"datasetVersion","datasetId":8028792},{"sourceId":12704801,"sourceType":"datasetVersion","datasetId":8029544},{"sourceId":511190,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":404889,"modelId":422790},{"sourceId":511191,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":404890,"modelId":422791},{"sourceId":511540,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":405083,"modelId":422976},{"sourceId":511542,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":405085,"modelId":422977}],"dockerImageVersionId":29838,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport joblib\n\n# 1. Load data\ndf = pd.read_csv('/kaggle/input/dataw2/benchmark_ml_features.csv')  # Replace with your actual file name\n\n# 2. Prepare features and labels\nX = df.drop(columns=['App', 'Label'])\ny = df['Label']\n\n# 3. Split data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 4. Train a classifier (Random Forest is robust for this kind of data)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluate the model\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n\n# Save the trained model to a file\n# joblib.dump(model, 'malware_detector_model.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:51:11.044856Z","iopub.execute_input":"2025-08-06T18:51:11.045159Z","iopub.status.idle":"2025-08-06T18:51:11.195545Z","shell.execute_reply.started":"2025-08-06T18:51:11.045103Z","shell.execute_reply":"2025-08-06T18:51:11.194787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn\nimport xgboost\nimport numpy\nimport cython\nimport scipy\nprint(sklearn.__version__)\nprint(xgboost.__version__)\nprint(numpy.__version__)\nprint(joblib.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:35:19.754700Z","iopub.execute_input":"2025-08-07T20:35:19.754986Z","iopub.status.idle":"2025-08-07T20:35:19.759843Z","shell.execute_reply.started":"2025-08-07T20:35:19.754941Z","shell.execute_reply":"2025-08-07T20:35:19.758889Z"}},"outputs":[{"name":"stdout","text":"0.21.3\n0.90\n1.16.4\n0.13.2\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport joblib\n# 1. Load\ndf = pd.read_csv('/kaggle/input/dataw2/benchmark_ml_features.csv')\nX = df.drop(['App', 'Label'], axis=1)\ny = df['Label']\n\n# 2. Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, stratify=y, random_state=42\n)\n\n# 3. Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\n\n# 4. SMOTE\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)\n\n# 5. XGBoost + GridSearch\nparam_grid = {\n    'max_depth': [3, 6, 10],\n    'learning_rate': [0.01, 0.1],\n    'n_estimators': [100, 300]\n}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(\n    XGBClassifier(\n        eval_metric='logloss',\n        use_label_encoder=False,\n        random_state=42,\n        n_jobs=-1\n    ),\n    param_grid,\n    scoring='f1',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\ngrid.fit(X_resampled, y_resampled)\nbest_model = grid.best_estimator_\n\n# 6. Evaluate\ny_pred = best_model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred, digits=3))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# 7. Threshold tuning\ny_proba = best_model.predict_proba(X_test_scaled)[:, 1]\nbest_f1, best_thr = 0, 0.5\nfor thr in [i / 100 for i in range(25, 76)]:\n    preds = (y_proba >= thr).astype(int)\n    score = f1_score(y_test, preds)\n    if score > best_f1:\n        best_f1, best_thr = score, thr\nprint(f'Best F1 {best_f1:.3f} at threshold {best_thr}')\n\n# joblib.dump(best_model, 'detect_malware_x.pkl')\n# joblib.dump(scaler, 'scaler_x.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T19:52:04.432412Z","iopub.execute_input":"2025-08-07T19:52:04.432740Z","iopub.status.idle":"2025-08-07T20:07:31.463121Z","shell.execute_reply.started":"2025-08-07T19:52:04.432661Z","shell.execute_reply":"2025-08-07T20:07:31.461849Z"}},"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"},{"name":"stdout","text":"Fitting 5 folds for each of 12 candidates, totalling 60 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  8.1min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 15.4min finished\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0      0.737     0.750     0.743        56\n           1      0.745     0.732     0.739        56\n\n    accuracy                          0.741       112\n   macro avg      0.741     0.741     0.741       112\nweighted avg      0.741     0.741     0.741       112\n\nConfusion matrix:\n [[42 14]\n [15 41]]\nBest F1 0.820 at threshold 0.33\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c297f237c5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# joblib.dump(best_model, 'detect_malware_x.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# joblib.dump(scaler, 'scaler_x.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'detect_malware_1_new.xgb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: save_model() takes 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"save_model() takes 2 positional arguments but 3 were given","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom lightgbm import LGBMClassifier\nimport joblib\n\n# 1. Load\ndf = pd.read_csv('/kaggle/input/dataw2/benchmark_ml_features.csv')\nX = df.drop(['App', 'Label'], axis=1)\ny = df['Label']\n\n# 2. Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, stratify=y, random_state=42\n)\n\n# 3. Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. SMOTE\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)\n\n# 5. LightGBM + GridSearch\nparam_grid = {\n    'max_depth': [3, 6, 10],\n    'learning_rate': [0.01, 0.1],\n    'n_estimators': [100, 300]\n}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(\n    LGBMClassifier(random_state=42, n_jobs=-1),\n    param_grid,\n    scoring='f1',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\ngrid.fit(X_resampled, y_resampled)\nbest_model = grid.best_estimator_\n\n# 6. Evaluate\ny_pred = best_model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred, digits=3))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# 7. Threshold tuning\ny_proba = best_model.predict_proba(X_test_scaled)[:, 1]\nbest_f1, best_thr = 0, 0.5\nfor thr in [i / 100 for i in range(25, 76)]:\n    preds = (y_proba >= thr).astype(int)\n    score = f1_score(y_test, preds)\n    if score > best_f1:\n        best_f1, best_thr = score, thr\nprint(f'Best F1 {best_f1:.3f} at threshold {best_thr}')\n\n# 8. Save\n# joblib.dump(best_model, 'detect_malware_lightgbm.pkl')\n# joblib.dump(scaler, 'scaler_lightgbm.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T04:48:40.242189Z","iopub.execute_input":"2025-08-08T04:48:40.242480Z","iopub.status.idle":"2025-08-08T05:00:01.845906Z","shell.execute_reply.started":"2025-08-08T04:48:40.242439Z","shell.execute_reply":"2025-08-08T05:00:01.844930Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 12 candidates, totalling 60 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  7.2min\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0      0.732     0.732     0.732        56\n           1      0.732     0.732     0.732        56\n\n    accuracy                          0.732       112\n   macro avg      0.732     0.732     0.732       112\nweighted avg      0.732     0.732     0.732       112\n\nConfusion matrix:\n [[41 15]\n [15 41]]\nBest F1 0.803 at threshold 0.33\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 11.4min finished\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport joblib\n\n# 1. Load\ndf = pd.read_excel('/kaggle/input/final-dataset/final_apis.xlsx')\nX = df.drop(['App', 'Label'], axis=1)\ny = df['Label']\n\n# Check initial data\nprint(\"Original X shape:\", X.shape)\nprint(\"Original y shape:\", y.shape)\nprint(\"NaN values in X:\\n\", X.isna().sum())\nprint(\"Infinite values in X:\", np.any(np.isinf(X)))\nprint(\"Unique values in y:\", y.unique())\n\n# Handle NaN and infinite values by filling with median\nX = X.replace([np.inf, -np.inf], np.nan)\nX = X.fillna(X.median())  # Fill NaN with column median\nprint(\"NaN values after filling:\\n\", X.isna().sum())\nprint(\"Infinite values after filling:\", np.any(np.isinf(X)))\nprint(\"X shape after cleaning:\", X.shape)\n\n# Remove low-variance columns\nvariances = X.var()\nlow_variance_cols = variances[variances < 1e-10].index\nX = X.drop(columns=low_variance_cols)\nprint(\"Dropped low-variance columns:\", low_variance_cols)\n\n# Ensure dataset is not empty\nif X.shape[0] == 0:\n    raise ValueError(\"Dataset is empty after cleaning. Check data for excessive NaN/infinite values.\")\n\n# 2. Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, stratify=y, random_state=42\n)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\n\n# 3. Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Check scaled data\nprint(\"NaN in X_train_scaled:\", np.any(np.isnan(X_train_scaled)))\nprint(\"Infinite in X_train_scaled:\", np.any(np.isinf(X_train_scaled)))\n\n# 4. SMOTE\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)\n\n# 5. XGBoost + GridSearch\nparam_grid = {\n    'max_depth': [3, 6, 10],\n    'learning_rate': [0.01, 0.1],\n    'n_estimators': [100, 300]\n}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(\n    XGBClassifier(\n        eval_metric='logloss',\n        random_state=42,\n        n_jobs=-1\n    ),\n    param_grid,\n    scoring='f1',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\ngrid.fit(X_resampled, y_resampled)\nbest_model = grid.best_estimator_\n\n# 6. Evaluate\ny_pred = best_model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred, digits=3))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# 7. Threshold tuning\ny_proba = best_model.predict_proba(X_test_scaled)[:, 1]\nbest_f1, best_thr = 0, 0.5\nfor thr in [i / 100 for i in range(25, 76)]:\n    preds = (y_proba >= thr).astype(int)\n    score = f1_score(y_test, preds)\n    if score > best_f1:\n        best_f1, best_thr = score, thr\nprint(f'Best F1 {best_f1:.3f} at threshold {best_thr}')\n\n# Save models\njoblib.dump(best_model, 'detect_malware_x.pkl')\njoblib.dump(scaler, 'scaler_x.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T04:12:48.412010Z","iopub.execute_input":"2025-08-08T04:12:48.412281Z","iopub.status.idle":"2025-08-08T04:20:16.025714Z","shell.execute_reply.started":"2025-08-08T04:12:48.412246Z","shell.execute_reply":"2025-08-08T04:20:16.024343Z"}},"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"},{"name":"stdout","text":"Original X shape: (224, 474)\nOriginal y shape: (224,)\nNaN values in X:\n Callback_onCreate                 0\nCallback_onDestroy                0\nCallback_onReceive                0\nCallback_onLowMemory              0\nCallback_onCreateContextMenu      0\n                               ... \nCallback_onDraw_ViewPager       179\nCallback_onLowMemory_2          179\nCallback_onMeasure_2            179\nCallback_onTouchEvent_2         179\nCallback_onDraw_2               179\nLength: 474, dtype: int64\nInfinite values in X: False\nUnique values in y: [0 1]\nNaN values after filling:\n Callback_onCreate               0\nCallback_onDestroy              0\nCallback_onReceive              0\nCallback_onLowMemory            0\nCallback_onCreateContextMenu    0\n                               ..\nCallback_onDraw_ViewPager       0\nCallback_onLowMemory_2          0\nCallback_onMeasure_2            0\nCallback_onTouchEvent_2         0\nCallback_onDraw_2               0\nLength: 474, dtype: int64\nInfinite values after filling: False\nX shape after cleaning: (224, 474)\nDropped low-variance columns: Index(['ICC_links', 'Callback_onSend', 'ICC_Links'], dtype='object')\nX_train shape: (112, 471)\nX_test shape: (112, 471)\nNaN in X_train_scaled: False\nInfinite in X_train_scaled: False\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-75e3485c8fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"best_model._Booster.save_model('detect_malware_1_new.xgb')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:07:48.741017Z","iopub.execute_input":"2025-08-07T20:07:48.741325Z","iopub.status.idle":"2025-08-07T20:07:48.746916Z","shell.execute_reply.started":"2025-08-07T20:07:48.741283Z","shell.execute_reply":"2025-08-07T20:07:48.745908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport numpy as np\n\n# List of features from your training set\nFEATURE_NAMES = [\n    \"VIDEO\", \"BLUETOOTH_INFORMATION\", \"CALENDAR_INFORMATION\", \"SMS_MMS\", \"ACCOUNT_INFORMATION\",\n    \"EMAIL_INFORMATION\", \"FILE_INFORMATION\", \"SYNCHRONIZATION_DATA\", \"PHONE_CONNECTION\", \"NETWORK\",\n    \"AUDIO\", \"IMAGE\", \"ACCOUNT_SETTINGS\", \"VOIP\", \"FILE\", \"DATABASE_INFORMATION\",\n    \"NETWORK_INFORMATION\", \"HARDWARE_INFO\", \"NFC\", \"SYSTEM_SETTINGS\", \"CONTACT_INFORMATION\",\n    \"VOIP_INFORMATION\", \"PHONE_INFORMATION\", \"EMAIL_SETTINGS\", \"PHONE_STATE\", \"BROWSER_INFORMATION\",\n    \"NO_CATEGORY\", \"LOG\", \"BLUETOOTH\", \"UNIQUE_IDENTIFIER\", \"INTER_APP_COMMUNICATION\",\n    \"EMAIL\", \"ALL\", \"LOCATION_INFORMATION\"\n]\n\ndef parse_inconsistent_src_txt(src_txt_path, feature_names):\n    \"\"\"\n    Parses a `src.txt` where each line is: `FEATURE_NAME val1 val2 val3 ...`\n    Extracts a single numeric summary (sum) per feature.\n\n    Args:\n        src_txt_path (str): Path to your src.txt\n        feature_names (list): List of features to consider.\n\n    Returns:\n        pd.DataFrame: DataFrame with one row containing feature values aligned to feature_names, or None if error.\n    \"\"\"\n    feature_dict = {feat: 0 for feat in feature_names}\n\n    try:\n        with open(src_txt_path, 'r', encoding='utf-8', errors='ignore') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    continue\n                feat_name = parts[0]\n                if feat_name not in feature_dict:\n                    print(f\"Warning: Unknown feature '{feat_name}' ignored.\")\n                    continue\n                # Convert the rest to numbers safely\n                try:\n                    values = [float(x) for x in parts[1:] if x.strip() != '']\n                    feature_dict[feat_name] = float(np.sum(values)) if values else 0.0\n                except ValueError as e:\n                    print(f\"Warning: Could not parse values for '{feat_name}': {e}. Setting to 0.\")\n                    feature_dict[feat_name] = 0.0\n    except Exception as e:\n        print(f\"Error parsing src.txt at '{src_txt_path}': {e}\")\n        return None\n\n    # Create DataFrame for model input\n    feature_df = pd.DataFrame([feature_dict])\n    # Ensure no NaN values\n    feature_df = feature_df.fillna(0)\n    return feature_df\n\n# --- SET THESE PATHS ---\nMODEL_PATH = '/kaggle/input/maal/scikitlearn/default/1/detect_malware_x.pkl'  # Path to your trained model\nSCALER_PATH = '/kaggle/input/scal/scikitlearn/default/1/scaler_x.pkl'         # Path to your scaler\nSRC_TXT_PATH = '/kaggle/input/malicious80/src.txt'                            # Path to your src.txt\n\n# --- PREDICTION CODE ---\ntry:\n    model = joblib.load(MODEL_PATH)\n    scaler = joblib.load(SCALER_PATH)\nexcept Exception as e:\n    print(f\"Error loading model or scaler: {e}\")\n    exit()\n\n# Parse features\nfv = parse_inconsistent_src_txt(SRC_TXT_PATH, FEATURE_NAMES)\n\n# Check if parsing was successful\nif fv is None or fv.empty:\n    print(\"Error: Feature vector is None or empty. Check src.txt file and parsing logic.\")\n    exit()\n\n# Verify the DataFrame\nprint(\"Parsed Features:\", fv)\n\n# Ensure the DataFrame has the correct shape\nif fv.shape[1] != len(FEATURE_NAMES):\n    print(f\"Error: Expected {len(FEATURE_NAMES)} features, got {fv.shape[1]}.\")\n    exit()\n\n# Scale and predict\ntry:\n    fv_scaled = scaler.transform(fv)\n    pred = model.predict(fv_scaled)[0]\n    proba = model.predict_proba(fv_scaled)[0][1]  # Probability of malware\n    print(f\"Prediction: {'Malicious' if pred == 1 else 'Benign'}\")\n    print(f\"Malicious probability: {proba:.4f}\")\nexcept Exception as e:\n    print(f\"Error during scaling or prediction: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T07:40:56.926946Z","iopub.execute_input":"2025-08-08T07:40:56.927206Z","iopub.status.idle":"2025-08-08T07:40:58.860912Z","shell.execute_reply.started":"2025-08-08T07:40:56.927165Z","shell.execute_reply":"2025-08-08T07:40:58.858701Z"}},"outputs":[{"name":"stdout","text":"Parsed Features:    VIDEO  BLUETOOTH_INFORMATION  CALENDAR_INFORMATION  SMS_MMS  \\\n0    9.0                    9.0                   9.0      9.0   \n\n   ACCOUNT_INFORMATION  EMAIL_INFORMATION  FILE_INFORMATION  \\\n0                  9.0                9.0               9.0   \n\n   SYNCHRONIZATION_DATA  PHONE_CONNECTION  NETWORK  ...  PHONE_STATE  \\\n0                   9.0               9.0      9.0  ...          9.0   \n\n   BROWSER_INFORMATION  NO_CATEGORY  LOG  BLUETOOTH  UNIQUE_IDENTIFIER  \\\n0                  9.0          9.0  9.0        9.0                9.0   \n\n   INTER_APP_COMMUNICATION  EMAIL  ALL  LOCATION_INFORMATION  \n0                      9.0    9.0  9.0                  17.0  \n\n[1 rows x 34 columns]\nPrediction: Malicious\nMalicious probability: 0.9720\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport numpy as np\n\n# List of features from your training set\nFEATURE_NAMES = [\n    \"VIDEO\", \"BLUETOOTH_INFORMATION\", \"CALENDAR_INFORMATION\", \"SMS_MMS\", \"ACCOUNT_INFORMATION\",\n    \"EMAIL_INFORMATION\", \"FILE_INFORMATION\", \"SYNCHRONIZATION_DATA\", \"PHONE_CONNECTION\", \"NETWORK\",\n    \"AUDIO\", \"IMAGE\", \"ACCOUNT_SETTINGS\", \"VOIP\", \"FILE\", \"DATABASE_INFORMATION\",\n    \"NETWORK_INFORMATION\", \"HARDWARE_INFO\", \"NFC\", \"SYSTEM_SETTINGS\", \"CONTACT_INFORMATION\",\n    \"VOIP_INFORMATION\", \"PHONE_INFORMATION\", \"EMAIL_SETTINGS\", \"PHONE_STATE\", \"BROWSER_INFORMATION\",\n    \"NO_CATEGORY\", \"LOG\", \"BLUETOOTH\", \"UNIQUE_IDENTIFIER\", \"INTER_APP_COMMUNICATION\",\n    \"EMAIL\", \"ALL\", \"LOCATION_INFORMATION\"\n]\n\ndef parse_inconsistent_src_txt(src_txt_path, feature_names):\n    \"\"\"\n    Parses a `src.txt` where each line is: `FEATURE_NAME val1 val2 val3 ...`\n    Extracts a single numeric summary (sum) per feature.\n\n    Args:\n        src_txt_path (str): Path to your src.txt\n        feature_names (list): List of features to consider.\n\n    Returns:\n        pd.DataFrame: DataFrame with one row containing feature values aligned to feature_names, or None if error.\n    \"\"\"\n    feature_dict = {feat: 0 for feat in feature_names}\n\n    try:\n        with open(src_txt_path, 'r', encoding='utf-8', errors='ignore') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    continue\n                feat_name = parts[0]\n                if feat_name not in feature_dict:\n                    print(f\"Warning: Unknown feature '{feat_name}' ignored.\")\n                    continue\n                # Convert the rest to numbers safely\n                try:\n                    values = [float(x) for x in parts[1:] if x.strip() != '']\n                    feature_dict[feat_name] = float(np.sum(values)) if values else 0.0\n                except ValueError as e:\n                    print(f\"Warning: Could not parse values for '{feat_name}': {e}. Setting to 0.\")\n                    feature_dict[feat_name] = 0.0\n    except Exception as e:\n        print(f\"Error parsing src.txt at '{src_txt_path}': {e}\")\n        return None\n\n    # Create DataFrame for model input\n    feature_df = pd.DataFrame([feature_dict])\n    # Ensure no NaN values\n    feature_df = feature_df.fillna(0)\n    return feature_df\n\n# --- SET THESE PATHS ---\nMODEL_PATH = '/kaggle/input/maal/scikitlearn/default/1/detect_malware_x.pkl'  # Path to your trained model\nSCALER_PATH = '/kaggle/input/scal/scikitlearn/default/1/scaler_x.pkl'         # Path to your scaler\nSRC_TXT_PATH = '/kaggle/input/source/src.txt'                            # Path to your src.txt\n\n# --- PREDICTION CODE ---\ntry:\n    model = joblib.load(MODEL_PATH)\n    scaler = joblib.load(SCALER_PATH)\nexcept Exception as e:\n    print(f\"Error loading model or scaler: {e}\")\n    exit()\n\n# Parse features\nfv = parse_inconsistent_src_txt(SRC_TXT_PATH, FEATURE_NAMES)\n\n# Check if parsing was successful\nif fv is None or fv.empty:\n    print(\"Error: Feature vector is None or empty. Check src.txt file and parsing logic.\")\n    exit()\n\n# Verify the DataFrame\nprint(\"Parsed Features:\", fv)\n\n# Ensure the DataFrame has the correct shape\nif fv.shape[1] != len(FEATURE_NAMES):\n    print(f\"Error: Expected {len(FEATURE_NAMES)} features, got {fv.shape[1]}.\")\n    exit()\n\n# Scale and predict\ntry:\n    fv_scaled = scaler.transform(fv)\n    pred = model.predict(fv_scaled)[0]\n    proba = model.predict_proba(fv_scaled)[0][1]  # Probability of malware\n    print(f\"Prediction: {'Malicious' if pred == 1 else 'Benign'}\")\n    print(f\"Malicious probability: {proba:.4f}\")\nexcept Exception as e:\n    print(f\"Error during scaling or prediction: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T07:41:30.122713Z","iopub.execute_input":"2025-08-08T07:41:30.122987Z","iopub.status.idle":"2025-08-08T07:41:30.176534Z","shell.execute_reply.started":"2025-08-08T07:41:30.122945Z","shell.execute_reply":"2025-08-08T07:41:30.174652Z"}},"outputs":[{"name":"stdout","text":"Parsed Features:    VIDEO  BLUETOOTH_INFORMATION  CALENDAR_INFORMATION  SMS_MMS  \\\n0    6.0                    6.0                   6.0      6.0   \n\n   ACCOUNT_INFORMATION  EMAIL_INFORMATION  FILE_INFORMATION  \\\n0                  6.0                6.0               6.0   \n\n   SYNCHRONIZATION_DATA  PHONE_CONNECTION  NETWORK  ...  PHONE_STATE  \\\n0                   6.0               6.0      6.0  ...          6.0   \n\n   BROWSER_INFORMATION  NO_CATEGORY  LOG  BLUETOOTH  UNIQUE_IDENTIFIER  \\\n0                  6.0          6.0  6.0        6.0                6.0   \n\n   INTER_APP_COMMUNICATION  EMAIL  ALL  LOCATION_INFORMATION  \n0                      6.0    6.0  6.0                   6.0  \n\n[1 rows x 34 columns]\nPrediction: Benign\nMalicious probability: 0.0315\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport joblib\n\n# 1. Load data\ndf = pd.read_excel('/kaggle/input/dataaaa/5144fb53-6896-41f6-89fe-d02170b14103.xlsx')\n\n# Handle both possible column names for app identifier\napp_col = 'App Name' if 'App Name' in df.columns else 'App'\nX = df.drop([app_col, 'Label'], axis=1)\ny = df['Label']\n\n# 2. Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, stratify=y, random_state=42\n)\n\n# 3. Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. SMOTE\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)\n\n# 5. XGBoost + GridSearch\nparam_grid = {\n    'max_depth': [3, 6, 10],\n    'learning_rate': [0.01, 0.1],\n    'n_estimators': [100, 300]\n}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(\n    XGBClassifier(\n        eval_metric='logloss',\n        use_label_encoder=False,\n        random_state=42,\n        n_jobs=-1\n    ),\n    param_grid,\n    scoring='f1',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\ngrid.fit(X_resampled, y_resampled)\nbest_model = grid.best_estimator_\n\n# 6. Evaluate\ny_pred = best_model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred, digits=3))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# 7. Threshold tuning\ny_proba = best_model.predict_proba(X_test_scaled)[:, 1]\nbest_f1, best_thr = 0, 0.5\nfor thr in [i / 100 for i in range(25, 76)]:\n    preds = (y_proba >= thr).astype(int)\n    score = f1_score(y_test, preds)\n    if score > best_f1:\n        best_f1, best_thr = score, thr\nprint(f'Best F1 {best_f1:.3f} at threshold {best_thr}')\n\n# 8. Save both model and scaler\njoblib.dump(best_model, 'detect_malware.pkl')\njoblib.dump(scaler, 'scaler.pkl')\n\n# 9. Save feature names for automation script\nfeature_names = list(X.columns)\nwith open('feature_names.txt', 'w') as f:\n    for feature in feature_names:\n        f.write(f\"{feature}\\n\")\n\nprint(\"‚úÖ Model saved as detect_malware.pkl\")\nprint(\"‚úÖ Scaler saved as scaler.pkl\") \nprint(\"‚úÖ Feature names saved as feature_names.txt\")\nprint(f\"‚úÖ Total features: {len(feature_names)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:46:07.578485Z","iopub.execute_input":"2025-08-07T10:46:07.578803Z","iopub.status.idle":"2025-08-07T11:03:09.791852Z","shell.execute_reply.started":"2025-08-07T10:46:07.578730Z","shell.execute_reply":"2025-08-07T11:03:09.790878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport joblib\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef train_malware_detection_model():\n    \"\"\"Complete training pipeline with comprehensive error handling\"\"\"\n    \n    # Configuration\n    DATA_PATH = '/kaggle/input/dataw2/benchmark_ml_features.csv'  # Update this path\n    SAVE_DIR = '/kaggle/working/'\n    \n    print(\"üöÄ Starting malware detection model training...\")\n    \n    # 1. Load and validate data\n    try:\n        if DATA_PATH.endswith('.xlsx'):\n            df = pd.read_excel(DATA_PATH)\n        else:\n            df = pd.read_csv(DATA_PATH)\n        print(f\"‚úÖ Data loaded successfully: {df.shape}\")\n    except Exception as e:\n        print(f\"‚ùå Failed to load data: {e}\")\n        return False\n    \n    # Handle different possible column names\n    app_column = None\n    label_column = None\n    \n    for col in df.columns:\n        if col.lower() in ['app', 'app name', 'app_name']:\n            app_column = col\n        elif col.lower() in ['label', 'class', 'target']:\n            label_column = col\n    \n    if not app_column or not label_column:\n        print(f\"‚ùå Required columns not found. Available columns: {list(df.columns)}\")\n        return False\n    \n    print(f\"üìä Using columns - App: '{app_column}', Label: '{label_column}'\")\n    \n    # Prepare features and target\n    X = df.drop([app_column, label_column], axis=1)\n    y = df[label_column]\n    \n    # Convert non-numeric columns to numeric\n    for col in X.columns:\n        if X[col].dtype == 'object':\n            try:\n                X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)\n            except:\n                X[col] = 0\n    \n    print(f\"üìà Feature matrix shape: {X.shape}\")\n    print(f\"üéØ Class distribution: {y.value_counts().to_dict()}\")\n    \n    # 2. Split data\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.25, stratify=y, random_state=42\n        )\n        print(\"‚úÖ Data split completed\")\n    except Exception as e:\n        print(f\"‚ùå Data splitting failed: {e}\")\n        return False\n    \n    # 3. Scale features\n    try:\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        print(\"‚úÖ Feature scaling completed\")\n    except Exception as e:\n        print(f\"‚ùå Feature scaling failed: {e}\")\n        return False\n    \n    # 4. Handle class imbalance with SMOTE\n    try:\n        smote = SMOTE(random_state=42)\n        X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n        print(f\"‚úÖ SMOTE applied - New shape: {X_resampled.shape}\")\n    except Exception as e:\n        print(f\"‚ùå SMOTE failed: {e}\")\n        return False\n    \n    # 5. Train model with grid search\n    try:\n        param_grid = {\n            'max_depth': [3, 6],\n            'learning_rate': [0.1, 0.2],\n            'n_estimators': [100, 200]\n        }\n        \n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n        \n        xgb_model = XGBClassifier(\n            eval_metric='logloss',\n            random_state=42,\n            n_jobs=1\n        )\n        \n        grid = GridSearchCV(\n            xgb_model,\n            param_grid,\n            scoring='f1',\n            cv=cv,\n            verbose=1,\n            n_jobs=1\n        )\n        \n        print(\"üîß Training model with grid search...\")\n        grid.fit(X_resampled, y_resampled)\n        best_model = grid.best_estimator_\n        print(f\"‚úÖ Model training completed. Best params: {grid.best_params_}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model training failed: {e}\")\n        return False\n    \n    # 6. Evaluate model\n    try:\n        y_pred = best_model.predict(X_test_scaled)\n        print(\"\\nüìä Model Evaluation:\")\n        print(classification_report(y_test, y_pred, digits=3))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(y_test, y_pred))\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Evaluation failed: {e}\")\n    \n    # 7. Save all components\n    try:\n        os.makedirs(SAVE_DIR, exist_ok=True)\n        \n        # Save model using XGBoost native format (most reliable)\n        model_path = os.path.join(SAVE_DIR, 'detect_malware_small.json')\n        best_model.save_model(model_path)\n        print(f\"‚úÖ Model saved: {model_path}\")\n        \n        # Save scaler\n        scaler_path = os.path.join(SAVE_DIR, 'scaler_small.pkl')\n        joblib.dump(scaler, scaler_path)\n        print(f\"‚úÖ Scaler saved: {scaler_path}\")\n        \n        # Save feature names\n        feature_names_path = os.path.join(SAVE_DIR, 'feature_names_small.txt')\n        with open(feature_names_path, 'w') as f:\n            for feature in X.columns:\n                f.write(f\"{feature}\\n\")\n        print(f\"‚úÖ Feature names saved: {feature_names_path}\")\n        \n        # Save metadata\n        metadata_path = os.path.join(SAVE_DIR, 'model_metadata.txt')\n        with open(metadata_path, 'w') as f:\n            f.write(f\"Model Type: XGBoost\\n\")\n            f.write(f\"Features: {len(X.columns)}\\n\")\n            f.write(f\"Training Samples: {len(X_train)}\\n\")\n            f.write(f\"Test Samples: {len(X_test)}\\n\")\n            f.write(f\"Best Parameters: {grid.best_params_}\\n\")\n        \n        print(\"üéâ All components saved successfully!\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to save components: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    success = train_malware_detection_model()\n    if success:\n        print(\"\\n‚úÖ Training pipeline completed successfully!\")\n    else:\n        print(\"\\n‚ùå Training pipeline failed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:38:50.735087Z","iopub.execute_input":"2025-08-07T12:38:50.735403Z","iopub.status.idle":"2025-08-07T12:38:50.912024Z","shell.execute_reply.started":"2025-08-07T12:38:50.735358Z","shell.execute_reply":"2025-08-07T12:38:50.910526Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d04d4f4ef99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mModule\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mallowing\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcreate\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mscikit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecate_parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_docstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubstitution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_neighbors_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# License: BSD 3 clause (C) INRIA, University of Amsterdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from ..utils import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_to_object_array'"],"ename":"ImportError","evalue":"cannot import name '_to_object_array'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"!pip install --upgrade numpy scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:16:49.270847Z","iopub.execute_input":"2025-08-07T12:16:49.271118Z","iopub.status.idle":"2025-08-07T12:17:06.660937Z","shell.execute_reply.started":"2025-08-07T12:16:49.271074Z","shell.execute_reply":"2025-08-07T12:17:06.660228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport xgboost as xgb\nimport re\nimport os\n\ndef load_feature_names(path):\n    with open(path, 'r') as f:\n        features = [line.strip() for line in f.readlines()]\n    return features\n\ndef extract_features_from_log(log_file_path, feature_names):\n    \"\"\"\n    Parse the log file and count occurrences of each lifecycle/callback method.\n    The feature names correspond to lifecycle or callback method names.\n\n    Args:\n        log_file_path (str): Path to the raw log file.\n        feature_names (list of str): List of feature names expected (e.g., 'onCreate', 'onPause', etc.)\n\n    Returns:\n        pd.DataFrame: One-row DataFrame where columns are feature names and values are counts.\n    \"\"\"\n    \n    # Initialize counts to zeros\n    feature_counts = {feature: 0 for feature in feature_names}\n    \n    # Compile regex patterns for features to speed up search (assuming feature names are method names)\n    feature_patterns = {feature: re.compile(r'\\b{}\\b'.format(re.escape(feature))) for feature in feature_names}\n    \n    with open(log_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n        for line in file:\n            for feature, pattern in feature_patterns.items():\n                if pattern.search(line):\n                    feature_counts[feature] += 1\n    \n    # Create a DataFrame with a single row\n    df_features = pd.DataFrame([feature_counts], columns=feature_names)\n    return df_features\n\ndef test_model_with_log(log_file_path):\n    # Paths to saved artifacts - update paths accordingly\n    model_path = '/kaggle/input/detect/scikitlearn/default/1/detect_malware_1.pkl'\n    scaler_path = '/kaggle/input/scaler/scikitlearn/default/1/scaler_1.pkl'\n    feature_names_path = '/kaggle/input/feature-names/feature_names.txt'\n    \n    # Check files exist\n    for path in [model_path, scaler_path, feature_names_path]:\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Required file not found: {path}\")\n    \n    # Load feature names\n    feature_names = load_feature_names(feature_names_path)\n    print(f\"Loaded {len(feature_names)} feature names.\")\n    \n    # Extract features from log file\n    X_new = extract_features_from_log(log_file_path, feature_names)\n    print(f\"Extracted features from log file with shape: {X_new.shape}\")\n    \n    # Handle any non-numeric columns (unlikely but safe)\n    for col in X_new.columns:\n        if X_new[col].dtype == 'object':\n            X_new[col] = pd.to_numeric(X_new[col], errors='coerce').fillna(0)\n    \n    # Load scaler and transform\n    scaler = joblib.load(scaler_path)\n    X_new_scaled = scaler.transform(X_new)\n    \n    # Load trained XGBoost model and predict\n    model = xgb.XGBClassifier()\n    model.load_model(model_path)\n    \n    prediction = model.predict(X_new_scaled)\n    prediction_proba = model.predict_proba(X_new_scaled)\n    \n    return prediction[0], prediction_proba[0]\n\nif __name__ == \"__main__\":\n    # Example usage - update path to your log file here\n    log_path = '/kaggle/input/security-report/security_report.log'\n    \n    try:\n        pred, proba = test_model_with_log(log_path)\n        print(f\"Prediction: {pred}\")\n        print(f\"Class probabilities: {proba}\")\n    except Exception as e:\n        print(f\"Error during testing: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:19:12.207668Z","iopub.execute_input":"2025-08-07T12:19:12.207896Z","iopub.status.idle":"2025-08-07T12:19:12.272262Z","shell.execute_reply.started":"2025-08-07T12:19:12.207849Z","shell.execute_reply":"2025-08-07T12:19:12.271631Z"}},"outputs":[{"name":"stdout","text":"Loaded 177 feature names.\nExtracted features from log file with shape: (1, 177)\nError during testing: cannot import name '_incremental_weighted_mean_and_var'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 1: Load trained model and scaler\nmodel = joblib.load('/kaggle/input/malware_detection/scikitlearn/default/1/detect_malware.pkl')\nsrc_txt_path = '/kaggle/input/malicious80/src.txt'\n# If you saved the scaler during training, load it here (recommended)\n# scaler = joblib.load('scaler.pkl')\n# But for now, we'll just recreate and fit it on training data again:\ndf = pd.read_csv('/kaggle/input/dataw2/benchmark_ml_features.csv')\nX_train = df.drop(columns=[\"App\", \"Label\"])\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# Step 2: Parse src.txt to count API calls\ndef parse_src_txt(file_path):\n    api_counts = {}\n    with open(file_path, \"r\") as f:\n        for line in f:\n            api = line.strip()\n            if api:\n                api_counts[api] = api_counts.get(api, 0) + 1\n    return api_counts\n\napi_count = parse_src_txt(src_txt_path)\n\n# Step 3: Align features to training data\nfeature_names = list(X_train.columns)\napp_features = [api_count.get(api, 0) for api in feature_names]\nX_input = pd.DataFrame([app_features], columns=feature_names)\n\n# Step 4: Scale\nX_input_scaled = scaler.transform(X_input)\n\n# Step 5: Predict\npred_class = model.predict(X_input_scaled)[0]\npred_proba = model.predict_proba(X_input_scaled)[0][1]\n\n# Optional: Use custom threshold\ncustom_threshold = 0.5\nfinal_prediction = \"Malicious\" if pred_proba >= custom_threshold else \"Benign\"\n\n# Output\nprint(\"Prediction:\", final_prediction)\nprint(\"Confidence (Malicious):\", f\"{pred_proba:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:51:36.868001Z","iopub.execute_input":"2025-08-06T18:51:36.868266Z","iopub.status.idle":"2025-08-06T18:51:36.938515Z","shell.execute_reply.started":"2025-08-06T18:51:36.868229Z","shell.execute_reply":"2025-08-06T18:51:36.937752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# === CONFIGURATION ===\nSRC_FILE_PATH = \"/kaggle/input/malicious80/src.txt\"  # üîÅ Replace this dynamically below\nMODEL_PATH = \"/kaggle/input/malware_detection/scikitlearn/default/1/detect_malware.pkl\"\nTRAIN_CSV = \"/kaggle/input/dataw2/benchmark_ml_features.csv\"\n\n# === 1. Load and Parse src.txt ===\ndef parse_src_txt_tab_sep(file_path):\n    df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n    method_counts = df[0].value_counts().to_dict()\n    return method_counts\n\napi_counts = parse_src_txt_tab_sep(SRC_FILE_PATH)\n\n# === 2. Align with Training Features ===\ntrain_df = pd.read_csv(TRAIN_CSV)\nfeature_columns = [col for col in train_df.columns if col not in [\"App\", \"Label\"]]\napp_features = [api_counts.get(col, 0) for col in feature_columns]\nX_input = pd.DataFrame([app_features], columns=feature_columns)\n\n# === 3. Scale ===\nscaler = StandardScaler()\nscaler.fit(train_df[feature_columns])\nX_scaled = scaler.transform(X_input)\n\n# === 4. Load Model and Predict ===\nmodel = joblib.load(MODEL_PATH)\npred = model.predict(X_scaled)[0]\nproba = model.predict_proba(X_scaled)[0][1]\n\n# === 5. Output ===\nprint(\"\\n--- üîç Prediction Result ---\")\nprint(f\"Prediction     : {'Malicious' if proba >= 0.5 else 'Benign'}\")\nprint(f\"Confidence     : {proba:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:51:55.716064Z","iopub.execute_input":"2025-08-06T18:51:55.716333Z","iopub.status.idle":"2025-08-06T18:51:55.781571Z","shell.execute_reply.started":"2025-08-06T18:51:55.716295Z","shell.execute_reply":"2025-08-06T18:51:55.780703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\n\nprint(\"last update: {}\".format(datetime.now())) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-06T19:31:06.830Z"}},"outputs":[],"execution_count":null}]}